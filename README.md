# Pretraining Graph Transformers

Implementation of pretraining methods for Graph Transformers. In particular for Graphormer. Moreover, an improved version of Graphormer3D that can utilize multiple node features. The pretained models perform superior to the models trained from scratch.

If you use anything from this repository than please cite

```bibtex
@misc{krauck_pretraining_2023,
    author = {Krauck, Alexander},
    title = {Pretraining Graph Transformers},
    year = {2023},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/alexanderkrauck/pretrained-graph-transformer}},
}
```
