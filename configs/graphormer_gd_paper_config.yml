pretraining: False
seed: 42
batch_size: 64
devices: [0, 1, 2, 3]
data_args:
  dataset_name: 'imdb'  # dataset name or path to dataset
  data_dir: 'data'  # directory to save the datasets
  max_seq_length: 256  # maximum sequence length for tokenization
  overwrite_cache: False  # whether to overwrite the cached preprocessed datasets
  cross_validation: True
  train_split: 0.8  # proportion of data for training, the rest is for validation/testing
trainer_args: 
  optim: "adam_hf" 
  gradient_accumulation_steps: 1
  learning_rate: 2e-4  # initial learning rate for optimizer
  weight_decay: 0.01  # weight decay if applicable
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8  # epsilon for Adam optimizer
  warmup_steps: 0  # number of warmup steps for learning rate scheduler
  max_grad_norm: 5.0
  logging_steps: 100  # number of steps between each log
  save_steps: 2000  # number of steps between each save
  lr_scheduler_type: "polynomial" #NOTE: if i want to set end-learning-rate, I need to create the scheduler myself, for now I will use the default of 1e-7
  warmup_steps: 60000
  num_train_epochs: 150
  evaluation_strategy: 'epoch'  # evaluation strategy to adopt during training (`'steps'`, `'epoch'`, `'interval'`)
  logging_strategy: 'epoch'
  do_eval: True  # whether to run evaluation during training
  save_total_limit: 4  # limit the total amount of checkpoints and deletes the older checkpoints
  dataloader_num_workers: 8 # number of subprocesses for data loading
  report_to: ['wandb', 'tensorboard']  # list of integrations to report the results and logs to
model_args:
  embedding_dim: 80
  ffn_embedding_dim: 80
  use_auth_token: False  # whether to use an auth token when using a private model
  num_attention_heads: 8  # number of attention heads for transformer model
  num_hidden_layers: 12  # number of hidden layers in the transformer encoder
  hidden_dropout_prob: 0.1  # dropout probability for all fully connected layers in the embeddings, encoder, and pooler
  attention_probs_dropout_prob: 0.1  # dropout probability for attention probabilities in transformer model
  dropout: 0.0
